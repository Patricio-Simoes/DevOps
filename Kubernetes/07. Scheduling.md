---
Date: 2025-01-05
Technology: Kubernetes
tags:
  - Container_Orchestration
  - DevOps
---
**Table of Contents**

- [07. Scheduling](#07-scheduling)
  - [Taints \& Tolerations](#taints--tolerations)
    - [Taints](#taints)
    - [Tolerations](#tolerations)
  - [Node Affinity \& Node Selectors](#node-affinity--node-selectors)
    - [Node Selectors](#node-selectors)
    - [Node Affinity](#node-affinity)
  - [Resource Limits](#resource-limits)
    - [LimitRange Object](#limitrange-object)
    - [ResourceQuota Object](#resourcequota-object)

# 07. Scheduling

Scheduling refers to the process of assigning workload requests, in the form of [pods](02.%20Pods.md#02.%20Pods), to specific [nodes](01.%20Overview.md#Nodes%20(Minions)) within a cluster.

## Taints & Tolerations

Taints & Tolerations are a mechanism in Kubernetes that **allow [nodes](01.%20Overview.md#Nodes%20(Minions)) to repel [pods](02.%20Pods.md#02.%20Pods)**.

<div align="center">
    <img src="../Assets/Images/Diagrams/Kubernetes/Taints & Tolerations.png" alt="Taints & Tolerations">
</div>

> For the pod to be available for creation on Node A, the following toleration would have to be applied:
> ```
> tolerations:
>   - key: app
>   - operator: "Equal" 
>   - value: nginx 
>   - effect: "NoSchedule"
>```

### Taints

A taint consists of three components:

- **Key**: A string that identifies the taint.
- **Value**: A string that complements the key but is optional.
- **Effect**: Defines what happens to [pods](02.%20Pods.md#02.%20Pods) that do not tolerate the taint. Possible effects are:
    - **NoSchedule**: New [pods](02.%20Pods.md#02.%20Pods) will not be scheduled on the [node](01.%20Overview.md#Nodes%20(Minions)).
    - **PreferNoSchedule**: The scheduler tries not to schedule [pods](02.%20Pods.md#02.%20Pods) on the [node](01.%20Overview.md#Nodes%20(Minions)) but may do so if no other options are available.
    - **NoExecute**: Existing [pods](02.%20Pods.md#02.%20Pods) without tolerations will be evicted from the [node](01.%20Overview.md#Nodes%20(Minions)).

When a [node](01.%20Overview.md#Nodes%20(Minions)) is tainted, it means that it will not accept any [pods](02.%20Pods.md#02.%20Pods) **unless they have a matching toleration**.

[Nodes](01.%20Overview.md#Nodes%20(Minions)) are tainted using the following `kubectl`command:

```bash
kubectl taint nodes <node> <key>=<value>:<effect>
# This is also a valid command:
kubectl taint node <node> <key>=<value>:<effect>
```

### Tolerations

Tolerations are the counterpart to taints. They **allow [pods](02.%20Pods.md#02.%20Pods) to be scheduled on nodes with taints**.

Tolerations are applied to [pods](02.%20Pods.md#02.%20Pods) by adding the following to their YAML specification:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  tolerations:
  - key: "key".          # Note: Needs to be wrapped in double quotes.
    operator: "Equal"    # Note: Needs to be wrapped in double quotes.
    value: "value"       # Note: Needs to be wrapped in double quotes.
    effect: "NoSchedule" # Note: Needs to be wrapped in double quotes.
```

## Node Affinity & Node Selectors

Node Affinity & Node Selectors are two mechanisms for **controlling where a [pod](02.%20Pods.md#02.%20Pods) should be scheduled**.

When combined with Taints & Tolerations, it is possible to know exactly where a pod will be scheduled to:

<div align="center">
    <img src="../Assets/Images/Diagrams/Kubernetes/Node Affinity & Node Selector.png" alt="Node Affinity & Node Selector">
</div>

### Node Selectors

Node selectors are the simplest of the two, with limited capabilities. 

Node selectors specify constraints based on labels assigned to nodes.

E.g.

First, label existing nodes with their disk type:

```bash
kubectl label nodes node1 disktype=ssd
kubectl label nodes node2 disktype=hdd
```

Second, apply the Node Selector to the YAML specification of the [pod](02.%20Pods.md#02.%20Pods) that should be scheduled to the SSD [node](01.%20Overview.md#Nodes%20(Minions)):

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  nodeSelector:
    disktype: ssd
  containers:
  - name: my-container
    image: nginx
```

**Note:** Node selectors are limited in their selection capabilities. For instance, if a pod **should not be scheduled** to a node, this cannot be defined using a Node Selector.

### Node Affinity

The primary purpose of the node affinity feature is to **ensure that [pods](02.%20Pods.md#02.%20Pods) are hosted on particular [nodes](01.%20Overview.md#Nodes%20(Minions))**.

There are two types of node affinity:

1. `RequiredDuringSchedulingIgnoredDuringExecution`: This means that a [pod](02.%20Pods.md#02.%20Pods) can only be scheduled on [nodes](01.%20Overview.md#Nodes%20(Minions)) that meet the specified criteria. If no qualifying [nodes](01.%20Overview.md#Nodes%20(Minions)) are available, the [pod](02.%20Pods.md#02.%20Pods) will not be scheduled.
2. `PreferredDuringSchedulingIgnoredDuringExecution`: This indicates that while the [pod](02.%20Pods.md#02.%20Pods) prefers to be scheduled on [nodes](01.%20Overview.md#Nodes%20(Minions)) that match the criteria, it can still be scheduled elsewhere if necessary. This type of affinity uses a weight to determine the preference.

Node affinity is defined within the YAML pod specification under `affinity` settings. E.g:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  nodeSelector:
    disktype: ssd
  containers:
  - name: my-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: NotIn
            values:
            - hdd
```

**Note:** Node Affinity does not ensure that other pods are not scheduled to a node, for that, Taints must be used.

## Resource Limits

When a [pod](02.%20Pods.md#02.%20Pods) is scheduled to a [node](01.%20Overview.md#Nodes%20(Minions)), in theory, it can consume all available resources until none are left.

To limit this, it is possible to set a cap on the amount of resources that should be available to that [pod](02.%20Pods.md#02.%20Pods). This is done in it's YAML the specification:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  nodeSelector:
    disktype: ssd
  containers:
  - name: my-container
    image: nginx
  resources:
    requests:
      memory: "64Mi"
      cpu: "250m"
    limits:
      memory: "128Mi"
      cpu: "500m"
```

The `limits`section sets a limit on how much resources the [pod](02.%20Pods.md#02.%20Pods) can consume.

> [!QUESTION] What happens when a container tries to exceed it's resource limit?
> In case of the CPU, the system throttles the CPU so that the container does not exceed it's imposed limit.
> In case of the memory however, a container can consume more memory than the imposed limit. If this happens often, the pod will be terminated with an Out Of Memory error in the logs.

The `requests`section defines the minimum amount of resources that need to be available on a [node](01.%20Overview.md#Nodes%20(Minions)) for the [pod](02.%20Pods.md#02.%20Pods) to be scheduled in it.

**Note:** See [Resource Units](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes) for a breakdown of the units used in the specification.

### LimitRange Object

To ensure that all [pods](02.%20Pods.md#02.%20Pods) have a default resource limit, no matter their YAML specification, LimitRange objects are used.

These objects **impose constraints on the resource requests and limits for [pods](02.%20Pods.md#02.%20Pods) or containers at [namespace](06.%20Namespaces.md#06.%20Namespaces) level**.

E.g. definition:

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-memory-limit-range
  namespace: example-namespace
spec:
  limits:
  - default: # Limit.
      cpu: "500m"
      memory: "256Mi"
    defaultRequest: # Request.
      cpu: "200m"
      memory: "128Mi"
    max: # Limit.
      cpu: "1000m"
      memory: "512Mi"
    min: # Request.
      cpu: "100m"
      memory: "64Mi"
    type: Container
```

### ResourceQuota Object

The ResourceQuota objects are used to set hard **limits on the amount of resources that are consumed by all of the deployed pods in total**.
06.%20Namespaces.md#06.%20Namespaces
ResourceQuotas are also applied at [namespace](06.%20Namespaces.md#06-namespaces) level.

E.g. definition:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-memory-quota
  namespace: example-namespace
spec:
  hard:
    requests.cpu: "2"
    requests.memory: "4Gi"
    limits.cpu: "4"
    limits.memory: "8Gi"
    persistentvolumeclaims: "5"
    pods: "10"
```

